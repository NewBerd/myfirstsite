<!DOCTYPE html>
<!-- saved from url=(0045)https://www.cnblogs.com/duanhx/p/9655223.html -->
<html lang="zh-cn">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>卷积神经网络</title>
</head>

<body>
<div>
<h2>卷积神经网络的基本原理</h2>

<h3>一、卷积神经网络构成</h3>

<p>&nbsp; &nbsp;卷积神经网络通常包含以下几种层：</p>
<ul>
    <li>卷积层（Convolutional layer），卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</li>
    <li>线性整流层（Rectified Linear Units layer, ReLU layer），这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）f(x)=max(0,x)f(x)=max(0,x)。</li>
    <li>池化层（Pooling layer），通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。</li>
    <li>全连接层（ Fully-Connected layer）, 把所有局部特征结合变成全局特征，用来计算最后每一类的得分。</li>
</ul>
<h3>二、卷积层（Convolutional layer）</h3>
<p>&nbsp; &nbsp; &nbsp; &nbsp;卷积层是卷积神经网络的核心基石。在图像识别里我们提到的卷积是二维卷积，即离散二维滤波器（也称作卷积核）与二维图像做卷积操作，简单的讲是二维滤波器滑动到二维图像上所有位置，并在每个位置上与该像素点及其领域像素点做内积。卷积操作被广泛应用与图像处理领域，不同卷积核可以提取不同的特征，例如边沿、线性、角等特征。在深层卷积神经网络中，通过卷积操作可以提取出图像低级到复杂的特征。</p>
<p><img class="has" src="/static/卷积神经网络_files/dl_3_2.png" alt="网络解析（一）：LeNet-5详解"></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;上图给出一个卷积计算过程的示例图，输入图像大小为H=5,W=5,D=3，即5×5大小的3通道（RGB，也称作深度）彩色图像。这个示例图中包含两（用K表示）组卷积核，即图中滤波器W0和W1。在卷积计算中，通常对不同的输入通道采用不同的卷积核，如图示例中每组卷积核包含（D=3）个3×3（用F×F表示）大小的卷积核。另外，这个示例中卷积核在图像的水平方向（W方向）和垂直方向（H方向）的滑动步长为2（用S表示）；对输入图像周围各填充1（用P表示）个0，即图中输入层原始数据为蓝色部分，灰色部分是进行了大小为1的扩展，用0来进行扩展。经过卷积操作得到输出为3×3×2（用Ho×Wo×K表示）大小的特征图，即3×3大小的2通道特征图，其中Ho计算公式为：Ho=(H−F+2×P)/S+1，Wo同理。 而输出特征图中的每个像素，是每组滤波器与输入图像每个特征图的内积再求和，再加上偏置bo，偏置通常对于每个输出特征图是共享的。输出特征图o[:,:,0]中的最后一个−2计算如上图右下角公式所示。</p>
<p>记住这几个符号：</p>
<ul>
<li>H：图片高度；</li>
<li>W：图片宽度；</li>
<li>D：原始图片通道数，也是卷积核个数；</li>
<li>F：卷积核高宽大小；</li>
<li>P：图像边扩充大小；</li>
<li>S：滑动步长。</li>
<li>K：&nbsp;深度，输出单元的深度</li>
</ul>
<p>&nbsp; &nbsp; &nbsp; &nbsp;在卷积操作中卷积核是可学习的参数，经过上面示例介绍，每层卷积的参数大小为D×F×F×K。卷积层的参数较少，这也是由卷积层的主要特性即局部连接和共享权重所决定。</p>
<ul>
<li>局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。</li>
<li>权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例上图中计算o[:,:,0]的每个每个神经元的滤波器均相同，都为W0，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。</li>
</ul>
<p>通过介绍卷积计算过程及其特性，可以看出卷积是线性操作，并具有平移不变性（shift-invariant），平移不变性即在图像每个位置执行相同的操作。卷积层的局部连接和权重共享使得需要学习的参数大大减小，这样也有利于训练较大卷积神经网络。</p>
<p>整体计算过程如下（与上图中的数据不同，但是计算过程相同）：</p>
<p><a href="/static/卷积神经网络_files/dl_3_12.gif" rel="nofollow" target="_blank"><img class="has" src="/static/卷积神经网络_files/dl_3_12.gif" alt="网络解析（一）：LeNet-5详解"></a></p>

<h3>三、线性整流层（Rectified Linear Units layer, ReLU layer）--激励层</h3><br/><p>&nbsp; &nbsp; &nbsp; &nbsp;把卷积层输出结果做非线性映射。</p>
<p><img class="has" src="/static/卷积神经网络_files/1093303-20170430194934006-705271151.jpg" alt=""></p>
<p><br>&nbsp; &nbsp; &nbsp; &nbsp;CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。</p>
<p><img class="has" src="/static/卷积神经网络_files/1093303-20170430194958725-2144325242.png" alt=""></p>
<p>&nbsp;</p>
<p>激励层的实践经验：<br>　　①不要用sigmoid！不要用sigmoid！不要用sigmoid！<br>　　② 首先试RELU，因为快，但要小心点<br>　　③ 如果2失效，请用Leaky ReLU或者Maxout<br>　　④ 某些情况下tanh倒是有不错的结果，但是很少</p>
<h3>四、池化层（Pooling layer）</h3>
<p>&nbsp; &nbsp; &nbsp; &nbsp;池化（pool）即下采样（downsamples），目的是为了减少特征图，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。通常在卷积层的后面会加上一个池化层。池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算，池化层进行的运算一般有以下几种：&nbsp;<br>&nbsp; &nbsp; &nbsp; * 最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。&nbsp;<br>&nbsp; &nbsp; &nbsp;* 均值池化（Mean Pooling）。取4个点的均值。&nbsp;<br>&nbsp; &nbsp; &nbsp;* 高斯池化。借鉴高斯模糊的方法。不常用。&nbsp;<br>&nbsp; &nbsp; &nbsp;* 可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。</p>
<p>最常见的池化层是规模为2*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示：&nbsp;</p>
<p><a href="/static/卷积神经网络_files/dl_3_3.png" rel="nofollow" target="_blank"><img class="has" src="/static/卷积神经网络_files/dl_3_3.png" alt="网络解析（一）：LeNet-5详解"></a></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;</p>
<p>&nbsp; &nbsp; &nbsp; 池化操作将保存深度大小不变。如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。</p><br/><br/><br/>
</div>
</body>
</html>